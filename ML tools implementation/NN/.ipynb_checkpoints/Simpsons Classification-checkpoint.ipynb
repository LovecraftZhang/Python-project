{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Simpsons!\n",
    "\n",
    "Let's build a neural network for recognizing simpson characters using keras. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration and Preprocessing\n",
    "Let's take a look at the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import random \n",
    "\n",
    "# For displaying images\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "\n",
    "mypath = 'simpson'\n",
    "onlyfiles = [mypath + \"/\" + f for f in listdir(mypath) if f.endswith(\".jpg\")]\n",
    "\n",
    "random.shuffle(onlyfiles)\n",
    "\n",
    "for file in onlyfiles[:10]:\n",
    "    img = Image(file)\n",
    "    display(img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Time to make arrays of our images, and encode our labels! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build our data and labels!\n",
    "import pprint\n",
    "\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import PIL.Image as Image\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "max_size = np.zeros(shape=(2,))\n",
    "\n",
    "data = list()\n",
    "labels = list()\n",
    "\n",
    "for file in onlyfiles[:350]:\n",
    "    \n",
    "    # Get image as a numpy array\n",
    "    im = Image.open(file)\n",
    "    \n",
    "    imarray = np.array(im)\n",
    "    #print(imarray.shape)\n",
    "    \n",
    "    # So we can pad the images so they are all the same size\n",
    "    if imarray.shape[0] > max_size[0]:\n",
    "        max_size[0] = imarray.shape[0]\n",
    "    \n",
    "    if imarray.shape[1] > max_size[1]:\n",
    "        max_size[1] = imarray.shape[1]\n",
    "    \n",
    "    # Get the labels\n",
    "    tokens = file.rsplit('_', 1)\n",
    "    label = tokens[0].split('/')[1]\n",
    "    #print(label)\n",
    "    \n",
    "    data.append(imarray)\n",
    "    labels.append(label)\n",
    "\n",
    "print('max width: %d, max height: %d' % (max_size[0], max_size[1]))    \n",
    "    \n",
    "# Pad the images to max width and max height\n",
    "data = np.asarray(data)\n",
    "new_data = list()\n",
    "for img in data:\n",
    "    img_new = np.zeros(shape=(int(max_size[0]), int(max_size[1]),3))\n",
    "    img_new[:img.shape[0],:img.shape[1],:] = img\n",
    "    new_data.append(img_new)\n",
    "\n",
    "new_data = np.asarray(new_data)\n",
    "print('data shape: ' + str(new_data.shape))\n",
    "\n",
    "# Build the label encoding\n",
    "lbl = 0\n",
    "label_map = dict()\n",
    "for label in labels:\n",
    "    if label not in label_map:\n",
    "        label_map[label] = lbl\n",
    "        lbl += 1\n",
    "\n",
    "encoded_labels = list()\n",
    "for label in labels:\n",
    "    encoded_labels.append(label_map[label])\n",
    "\n",
    "bin_labels = to_categorical(encoded_labels)\n",
    "\n",
    "# Split data into Training, Validation, and Test\n",
    "num_dat = len(encoded_labels)\n",
    "\n",
    "train_data, train_labels = new_data[: math.floor(0.75 * num_dat)], bin_labels[: math.floor(0.75 * num_dat)]\n",
    "valid_data, valid_labels = new_data[math.floor(0.75 * num_dat): math.floor(0.9 * num_dat)], bin_labels[math.floor(0.75 * num_dat): math.floor(0.9 * num_dat)]\n",
    "test_data, test_labels = new_data[math.floor(0.9 * num_dat):], bin_labels[math.floor(0.9 * num_dat):]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to build our model\n",
    "\n",
    "Now that we have our data, let's build the neural model. \n",
    "We're going to construct a Convolutional Neural Network. They're especially good at classifying images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports..\n",
    "from keras.models import Sequential # Allows us to modularly add layers with ease.\n",
    "from keras.layers import Conv2D, Activation, Dense, Flatten\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import losses\n",
    "from keras import optimizers\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers to the model\n",
    "\n",
    "model.add(\n",
    "    Conv2D(\n",
    "        filters=10,\n",
    "        kernel_size=5,\n",
    "        activation='linear',\n",
    "        strides=1,\n",
    "        input_shape=(int(max_size[0]), int(max_size[1]), 3)\n",
    "    )\n",
    ")\n",
    "\n",
    "model.add(\n",
    "    BatchNormalization()\n",
    ")\n",
    "\n",
    "model.add(\n",
    "    Activation('tanh')\n",
    ")\n",
    "\n",
    "model.add(\n",
    "    Dropout(\n",
    "        rate=0.2\n",
    "    )\n",
    ")\n",
    "\n",
    "model.add(\n",
    "    Flatten()\n",
    ")\n",
    "\n",
    "model.add(\n",
    "    Dense(\n",
    "        128, \n",
    "        activation='linear'\n",
    "    )\n",
    ")\n",
    "\n",
    "model.add(\n",
    "    BatchNormalization()\n",
    ")\n",
    "\n",
    "model.add(\n",
    "    Activation('relu')\n",
    ")\n",
    "\n",
    "model.add(\n",
    "    Dropout(\n",
    "        rate=0.2\n",
    "    )\n",
    ")\n",
    "\n",
    "model.add(\n",
    "    Dense(\n",
    "        lbl, \n",
    "        activation='softmax'\n",
    "    )\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss=losses.categorical_crossentropy,\n",
    "    optimizer=optimizers.Adadelta(),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation!\n",
    "Now that we have our model, it's time for training and validation phase. We'll just train for 30 epochs and look at the validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train data shape: ' + str(train_data.shape) + ', Train labels shape: ' + str(train_labels.shape))\n",
    "print('Valid data shape: ' + str(valid_data.shape) + ', Valid labels shape: ' + str(valid_labels.shape))\n",
    "print('Test data shape: ' + str(test_data.shape) + ', Test labels shape: ' + str(test_labels.shape))\n",
    "\n",
    "# Fit the model.\n",
    "\n",
    "history = model.fit(\n",
    "    x=train_data,\n",
    "    y=train_labels,\n",
    "    epochs=30,\n",
    "    verbose=1,\n",
    "    validation_data=(valid_data, valid_labels)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "Now, let's see what the net says on our test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
